---
title: "Paper script"
author: "Helio"
date: "2024-07-28"
output: html_document
---



preprocessign script - preprocess_halofacestudy.Rmd (keep separate)

Spatiotemporal components
posed: halo_facestudy_analysis.Rmd
- fit NMF

Spatiotemporal components


for the actual analyses we want
1- learn spatiotemporal structure suing NMF
2 - does drug vs no drug differ in NMF metrics?
3- can we separate groups based on NMF components using BADIA

spatiotemporal dynamics
- movement substates (initiation, sustainement, etc)
- if we use kmean clustering, do we have differences into how many substaes are there
- or if we use a fixed number of substaes, are there significant temporal differences (e.g. duration, reccurence)

preferences

load("~/Library/CloudStorage/GoogleDrive-helioclemente.c@gmail.com/My Drive/2022 - University Of Birmingham/HaloStudy/Data/halofacestudyOCT2023.RData")
```{r}
library(tidyverse)
library(data.table)
library(NMF)
# install.packages("ggforce")
require(ggforce)

library(magick)
# install.packages('imager')
# library(imager)

# library(png)
# library(grid)


require(ggtext)
library(caret)

```


Steps and decision
initialisation = random seed
find k
set iterations
fit final
# Initialise and find k
```{r}
colnames(df_OF_output_AUsW_unblind_posed_binned)


df_OF_output_AUsW_unblind_posed_binned

# single run and single numeric seed for reproducibility
res_find_k <- nmf(df_OF_output_AUsW_unblind_posed_binned[,16:32], 2:6, 
                      seed=123456,
                      nrun = 100,
                       .options = 'v')

# plot quality measures by rank (k)
library(tidyverse)
test<- NMF::plot(res_find_k)
  geom_vline(xintercept = 3)

test$data
  
# ?NMF::consensusmap
NMF::consensusmap(res_find_k)

colnames(df_OF_output_AUsW_unblind_posed_binned)
```

We select values of k where the magnitude of the cophenetic correlation coefficient begins to fall (see below).
so now we can fit the final model

between 3 and 4

Check for overfiting by reshufling the ata and refitting the NMF on simulated data
SHUFFLING to avoid overfiting to noise

```{r}
# shuffle original data 
colnames(df_OF_output_AUsW_unblind_posed_binned[,16:32])

V.random <- randomize(df_OF_output_AUsW_unblind_posed_binned[,16:32]) 

# estimate quality measures from the shuffled data (use default NMF algorithm) 
estim.k.random <- nmf(V.random, 2:6, nrun=100, seed=123456,
                      .option = 'v') 



# then we can assess the quality of random estimation to our estimation

# plot measures on same graph (x, y) 
plot(res_k3, estim.k.random) + 
  geom_vline(xintercept = 3)



# exhausting memory so try on a subset

# Set seed for reproducibility
set.seed(123456)

# Define the size of the subset (e.g., 50% of the original data)
subset_size <- 0.5



# Create a vector of unique timeseries IDs
unique_timeseries_ids <- unique(df_OF_output_AUsW_unblind_posed_binned$filename)

# Helper function to perform stratified sampling
stratified_sampling <- function(data, group_col, size) {
  data %>%
    group_by(!!sym(group_col)) %>%
    sample_frac(size = size / n(), replace = FALSE) %>%
    ungroup()
}

# Perform stratified sampling to select unique timeseries IDs based on the 'expression' column
stratified_ids <- df_OF_output_AUsW_unblind_posed_binned %>%
  group_by(expression) %>%
  summarise(filename = list(unique(filename))) %>%
  unnest(filename) %>%
  group_by(expression) %>%
  mutate(sample_id = row_number()) %>%
  ungroup() %>%
  filter(sample_id <= subset_size * n() / length(unique(expression))) %>%
  pull(filename)

# Split the data based on the selected timeseries IDs
dta_subs_a <- df_OF_output_AUsW_unblind_posed_binned %>%
  filter(filename %in% stratified_ids)

table(dta_subs_a$expression)

dta_subs_b <- df_OF_output_AUsW_unblind_posed_binned %>%
  filter(!filename %in% stratified_ids)



colnames(dta_subs_a)

res_find_k_1 <- NMF::nmf(dta_subs_a[,16:32], 2:6, 
                      seed=123456,
                      nrun = 10,
                       .options = 'v')



```

# oprtion 1 - compleetly destroy tenmporal spartiotemporal dependencies in the data but within each timeseries

# Randomising Temporal Structure:
# 
# Completely shuffle the time series data within each group to destroy temporal dependencies.
# Randomising Spatial Structure:
# 
# Shuffle the spatial features independently to destroy spatial dependencies.
For spatiotemporal NMF analysis, the block permutation method is more appropriate because:

It creates a null model that still has some spatiotemporal structure, but with disrupted larger-scale patterns.
It allows you to test whether your NMF components are capturing patterns beyond what would be expected from local, short-term correlations.
It's a more conservative approach, making your results more robust if you find differences between the original and randomized data.

```{r}




# Function to create a fully randomised dataset
# Function to create a fully randomised dataset

fn_spatiotemporal_randomize <- function(X, groups, 
                                        time_block_size = 10, 
                                        space_block_size = 5) {
  X_randomised <- X
  unique_groups <- unique(groups)
  n_features <- ncol(X)
  
  for (group in unique_groups) {
    indices <- which(groups == group)
    n_samples <- length(indices)
    
    # Temporal block permutation
    n_time_blocks <- floor(n_samples / time_block_size)
    if (n_time_blocks > 1) {
      time_block_indices <- split(indices, rep(1:n_time_blocks, each = time_block_size, length.out = n_samples))
      shuffled_time_blocks <- sample(time_block_indices)
      new_time_indices <- unlist(shuffled_time_blocks)
      X_randomised[indices, ] <- X[new_time_indices, ]
    
    
    # Spatial block permutation
    # n_space_blocks <- floor(n_features / space_block_size)
    # if (n_space_blocks > 1) {
    #   for (i in 1:n_time_blocks) {
    #     block_start <- (i - 1) * time_block_size + 1
    #     block_end <- min(i * time_block_size, n_samples)
    #     
    #     space_block_indices <- split(1:n_features, rep(1:n_space_blocks, each = space_block_size, length.out = n_features))
    #     shuffled_space_blocks <- sample(space_block_indices)
    #     new_space_indices <- unlist(shuffled_space_blocks)
    #     
        # X_randomised[indices[block_start:block_end], ] <- X_randomised[indices[block_start:block_end], new_space_indices]
      }
    }
  
  
  return(X_randomised)
}

# Usage:
dta_posed_subs_a_rnd <- fn_spatiotemporal_randomize(
  dta_subs_a[,16:32],
  dta_subs_a$filename,
  time_block_size = 10,
  space_block_size = 3  # Adjust based on number of spatial features
)




dta_posed_subs_a_rnd$filename <- dta_subs_a$filename
dta_posed_subs_a_rnd$expression <- dta_subs_a$expression
dta_posed_subs_a_rnd$timestamp <- dta_subs_a$timestamp




dta_subs_a%>%
  ggplot(aes(timestamp,AU06_r_Cheek_raiser, group = filename))+
  geom_line(colour = "blue", alpha = .5)+
  geom_line(aes(y = AU01_r_Inner_brow_raiser), colour = "red", alpha = .1)+
  facet_grid(~expression)



dta_posed_subs_a_rnd%>%
  ggplot(aes(timestamp,AU06_r_Cheek_raiser, group = filename))+
  geom_line(colour = "blue", alpha = .5)+
  geom_line(aes(y = AU01_r_Inner_brow_raiser), colour = "red", alpha = .1)+
  facet_grid(~expression)


colnames(dta_posed_rnd)




# estimate quality measures from the shuffled data (use default NMF algorithm)
estim_posed_random_1 <- NMF::nmf(dta_posed_subs_a_rnd[,1:17], 2:6, nrun=10, seed=123456)


plot(res_find_k_1)


plot(res_find_k_1, estim_posed_random_1)+
  geom_vline(xintercept = 3)

?NMF::consensusmap
NMF::consensusmap(res_find_k_1)

NMF::consensusmap(estim_posed_random_1)



# Compare components
res_find_k_1[,1]


original_components<- NMF::basis(res_find_k_1$fit$`3`)
original_components

random_components<- NMF::basis(estim_posed_random_1$fit$`3`)  


mean(abs(cor(original_components, random_components)))
mean(abs(cor(NMF::basis(res_find_k_1$fit$`2`), NMF::basis(estim_posed_random_1$fit$`2`) )))
mean(abs(cor(NMF::basis(res_find_k_1$fit$`4`), NMF::basis(estim_posed_random_1$fit$`4`) )))
mean(abs(cor(NMF::basis(res_find_k_1$fit$`5`), NMF::basis(estim_posed_random_1$fit$`5`) )))
mean(abs(cor(NMF::basis(res_find_k_1$fit$`6`), NMF::basis(estim_posed_random_1$fit$`6`) )))


dta_posed_subs_a_rnd[,1:17]


```




Max-normalizing before fitting NMF:

Pros:

Ensures all features are on the same scale, preventing features with larger magnitudes from dominating the NMF.
Can help with numerical stability during the NMF algorithm's iterations.
May lead to more interpretable components, as the contributions of different features are more comparable.

Cons:

May alter the relative relationships between features within each sample.
Could potentially mask important amplitude differences between features.


Normalizing the components after fitting NMF:

Pros:

Preserves the original scale relationships between features in the data.
Allows the NMF to capture amplitude differences between features, which might be important in your analysis.
Can still provide interpretable results if you're more interested in relative patterns than absolute magnitudes.

Cons:

Features with larger magnitudes might dominate the NMF results.
May need to be careful about numerical stability during NMF fitting, especially if your data has a wide range of values.

Recommendation:
In many cases, especially for spatiotemporal data like yours, normalizing after fitting NMF is often preferred. Here's why:

Preserves original relationships: For spatiotemporal data, the relative magnitudes between different features (in your case, AUs) might carry important information.
Captures amplitude differences: Different AUs might naturally have different ranges of activation, which could be important for your analysis.
Post-normalization interpretability: You can still normalize the components after fitting to make them more comparable and interpretable, without losing the benefits of fitting on the original scale.

library(stats)

auto size

```{r}

library(stats)

# Function to determine appropriate time block size based on ACF threshold
acf_threshold = .3
# tgis is potentiall problemayic as it doesnt shuffle the dataa enough asits too data centric
# auto_time_block_size <- function(X, groups, max_lag = 15, acf_threshold = .3) {
#   unique_groups <- unique(groups)
#   
#   acf_results <- lapply(unique_groups, function(group) {
#     indices <- which(groups == group)
#     group_data <- X[indices, ]
#     acf_mean <- colMeans(apply(group_data, 2, function(col) acf(col, lag.max = max_lag, plot = FALSE)$acf))
#     block_size <- which(acf_mean < acf_threshold)[1]
#     if (is.na(block_size)) max_lag else block_size
#   })
#   
#   time_block_size <- max(unlist(acf_results), na.rm = TRUE)
#   if (is.infinite(time_block_size) || is.na(time_block_size)) {
#     time_block_size <- max_lag
#   }
#   
#   return(time_block_size)
# }
# 
# # Function for temporal block permutation
# fn_temporal_block_permute <- function(X, groups, time_block_size = NULL) {
#   if (is.null(time_block_size)) {
#     time_block_size <- auto_time_block_size(X, groups)
#   }
#   
#   X_permuted <- X
#   unique_groups <- unique(groups)
#   
#   for (group in unique_groups) {
#     indices <- which(groups == group)
#     n_samples <- length(indices)
#     
#     if (n_samples > 1) {
#       # Perform block permutation
#       n_blocks <- floor(n_samples / time_block_size)
#       if (n_blocks > 1) {
#         block_indices <- split(1:n_samples, rep(1:n_blocks, each = time_block_size, length.out = n_samples))
#         permuted_order <- unlist(sample(block_indices))
#         X_permuted[indices, ] <- X[indices[permuted_order], ]
#       } else {
#         # If only one block, permute all time points
#         X_permuted[indices, ] <- X[indices[sample(n_samples)], ]
#       }
#     }
#   }
#   
#   return(X_permuted)
# }


library(stats)

library(stats)

# Function for temporal block permutation with fixed block size
fn_temporal_block_permute <- function(X, groups, time_block_size) {
  X_permuted <- X
  unique_groups <- unique(groups)
  
  for (group in unique_groups) {
    indices <- which(groups == group)
    n_samples <- length(indices)
    
    if (n_samples > 1) {
      # Perform block permutation for each column independently
      n_blocks <- floor(n_samples / time_block_size)
      if (n_blocks > 1) {
        for (col in 1:ncol(X)) {
          block_indices <- split(1:n_samples, rep(1:n_blocks, each = time_block_size, length.out = n_samples))
          permuted_order <- unlist(sample(block_indices))
          X_permuted[indices, col] <- X[indices[permuted_order], col]
        }
      } else {
        # If only one block, permute all time points for each column
        for (col in 1:ncol(X)) {
          X_permuted[indices, col] <- X[indices[sample(n_samples)], col]
        }
      }
    }
  }
  
  return(X_permuted)
}

# Function to run permutation test with different block sizes
# permutation_sensitivity <- function(X, groups, block_sizes = c(1, 5, 10, 15, 20), n_permutations = 100) {
#   results <- lapply(block_sizes, function(size) {
#     perms <- replicate(n_permutations, fn_temporal_block_permute(X, groups, size), simplify = FALSE)
#     
#     # Calculate your test statistic for each permutation
#     # This is a placeholder - replace with your actual test statistic calculation
#     test_stats <- sapply(perms, function(perm) apply(perm, 2, sum))
#     
#     # Calculate observed test statistic
#     observed_stats <- apply(X, 2, sum)
#     
#     # Calculate p-values for each column
#     p_values <- rowMeans(abs(test_stats) >= abs(observed_stats))
#     
#     return(list(block_size = size, p_values = p_values))
#   })
#   
#   return(results)
# }


# Example usage:



# Example usage:
# dta_subs_a <- matrix(rnorm(1000), ncol = 10)
# groups <- rep(1:5, each = 20)
# time_block_size_sensitivity_rs <- permutation_sensitivity(dta_subs_a, groups)
# for (result in time_block_size_sensitivity_rs) {
#   cat("Block size:", result$block_size, "- p-value:", result$p_value, "\n")
# }


colnames(dta_subs_a)
fn_temporal_block_permute()


 dta_posed_subs_a_rnd_2_tbs_5<- fn_temporal_block_permute(dta_subs_a[,16:32], 
                                                          dta_subs_a$filename,
                                                     time_block_size = 5)
 
dta_posed_subs_a_rnd_2_tbs_10<- fn_temporal_block_permute(dta_subs_a[,16:32], 
                                                          dta_subs_a$filename,
                                                     time_block_size = 10)
  
  
  dta_posed_subs_a_rnd_2_tbs_15<- fn_temporal_block_permute(dta_subs_a[,16:32], 
                                                          dta_subs_a$filename,
                                                     time_block_size = 15)






dta_posed_subs_a_rnd_2_tbs_5$filename <- dta_subs_a$filename
dta_posed_subs_a_rnd_2_tbs_5$expression <- dta_subs_a$expression
dta_posed_subs_a_rnd_2_tbs_5$timestamp <- dta_subs_a$timestamp


dta_posed_subs_a_rnd_2_tbs_10$filename <- dta_subs_a$filename
dta_posed_subs_a_rnd_2_tbs_10$expression <- dta_subs_a$expression
dta_posed_subs_a_rnd_2_tbs_10$timestamp <- dta_subs_a$timestamp

dta_posed_subs_a_rnd_2_tbs_15$filename <- dta_subs_a$filename
dta_posed_subs_a_rnd_2_tbs_15$expression <- dta_subs_a$expression
dta_posed_subs_a_rnd_2_tbs_15$timestamp <- dta_subs_a$timestamp





dta_subs_a%>%
  ggplot(aes(timestamp,AU06_r_Cheek_raiser, group = filename))+
  geom_line(colour = "blue", alpha = .5)+
  geom_line(aes(y = AU01_r_Inner_brow_raiser), colour = "red", alpha = .1)+
  facet_grid(~expression)

dta_posed_subs_a_rnd_2_tbs_5%>%
  ggplot(aes(timestamp,AU06_r_Cheek_raiser, group = filename))+
  geom_line(colour = "blue", alpha = .5)+
  geom_line(aes(y = AU01_r_Inner_brow_raiser), colour = "red", alpha = .1)+
  facet_grid(~expression)

dta_posed_subs_a_rnd_2_tbs_10%>%
  ggplot(aes(timestamp,AU06_r_Cheek_raiser, group = filename))+
  geom_line(colour = "blue", alpha = .5)+
  geom_line(aes(y = AU01_r_Inner_brow_raiser), colour = "red", alpha = .1)+
  facet_grid(~expression)

dta_posed_subs_a_rnd_2_tbs_15%>%
  ggplot(aes(timestamp,AU06_r_Cheek_raiser, group = filename))+
  geom_line(colour = "blue", alpha = .5)+
  geom_line(aes(y = AU01_r_Inner_brow_raiser), colour = "red", alpha = .1)+
  facet_grid(~expression)

```



```{r}
# estimate quality measures from the shuffled data (use default NMF algorithm)
estim_posed_random_2_tbs_10 <- NMF::nmf(dta_posed_subs_a_rnd_2_tbs_10[,1:17], 
                                 2:6, nrun=10, seed=123456)

estim_posed_random_2_tbs_5 <- NMF::nmf(dta_posed_subs_a_rnd_2_tbs_5[,1:17], 
                                 2:6, nrun=10, seed=123456)


estim_posed_random_2_tbs_15 <- NMF::nmf(dta_posed_subs_a_rnd_2_tbs_15[,1:17], 
                                 2:6, nrun=10, seed=123456)


plot(res_find_k_1)

plot(res_find_k_1,estim_posed_random_2_tbs_5)+
  geom_vline(xintercept = 3)

plot(res_find_k_1,estim_posed_random_2_tbs_10)+
  geom_vline(xintercept = 3)

plot(res_find_k_1,estim_posed_random_2_tbs_15)+
  geom_vline(xintercept = 3)



?NMF::randomize()

NMF::consensusmap(res_find_k_1)
NMF::consensusmap(estim_posed_random_2)
?NMF::consensusmap

cons<-NMF::consensusmap(res_find_k_1)
NMF::consensusmap(estim_posed_random_2_tbs_10)


mean(abs(cor(NMF::basis(res_find_k_1$fit$`2`), NMF::basis(estim_posed_random_2_tbs_10$fit$`2`) )))
mean(abs(cor(NMF::basis(res_find_k_1$fit$`3`), NMF::basis(estim_posed_random_2_tbs_10$fit$`3`) )))
mean(abs(cor(NMF::basis(res_find_k_1$fit$`4`), NMF::basis(estim_posed_random_2_tbs_10$fit$`4`) )))
mean(abs(cor(NMF::basis(res_find_k_1$fit$`5`), NMF::basis(estim_posed_random_2_tbs_10$fit$`5`) )))
mean(abs(cor(NMF::basis(res_find_k_1$fit$`6`), NMF::basis(estim_posed_random_2_tbs_10$fit$`6`) )))


dta_posed_subs_a_rnd[,1:17]

# ALL GOOD

```




# Note: This randomisation still stores enough information for the NMF, 
# and since it's not a true spatiotemporal NMF, the metrics still fit.
# Therefore, the auto-sizing may not work that well.
#
# Also, consider just applying NMF randomisation functions directly per group,
# as it would destroy the spatiotemporal structure.
# The effect of the randomisation on the spatiotemporal structure is only likely visible
# at a later stage when you take the features for classification.
# This model likely won't be able to classify things based on spatiotemporal features.
We select values of k where the magnitude of the cophenetic correlation coefficient begins to fall (see below).
so now we can fit the final model

```{r}

colnames(df_OF_output_AUsW_unblind_posed_binned)
df_OF_output_AUsW_unblind_posed_binned$AU01_r_Inner_brow_raiser

res_k3 <- NMF::nmf(df_OF_output_AUsW_unblind_posed_binned[,16:32], r = 3, 
                  nrun = 200, 
                  seed=123456,
                 .options = list( 'v')) #


```




store NMF components on dataset
```{r}

# reconstruct

res_k3

install.packages(NMF)


df_OF_output_AUsW_unblind_posed_binned$NMFtable<- as.data.frame(res_k3@fit@W)

df_OF_output_AUsW_unblind_posed_binned$comp1 = df_OF_output_AUsW_unblind_posed_binned$NMFtable$V1
table(is.na(df_OF_output_AUsW_unblind_posed_binned$comp1))
df_OF_output_AUsW_unblind_posed_binned$comp2 = df_OF_output_AUsW_unblind_posed_binned$NMFtable$V2
df_OF_output_AUsW_unblind_posed_binned$comp3 = df_OF_output_AUsW_unblind_posed_binned$NMFtable$V3




```



CONTINUE HERE - ORGANISE  CODE BELOW FOR FIGURE 2

Combine the data and visualise
vis preferrences 
Figure 2

```{r}

sf = 1 # scaling factor to make it easier to change sizes of everything, in which case change here

p<- list()

p$graphstyle_int <-  theme(#base plot theme
  
  # axis lines
  axis.line.y = element_line(color="black", size = 1.5),
  axis.line.x = element_line(color="black", size = 1,5),
  
  axis.title.y=element_text(size = 16*(sf+.5), margin=margin(0,5,0,0)),
  axis.title.x=element_text(size = 16*(sf+.5), margin=margin(0,5,0,0)),
  
  # text
  strip.text.x = element_text(size = 16*(sf+.5),  colour = "black"),
  strip.text.y = element_text(size = 16*(sf+.5),  colour = "black"),
  
  text=element_text(size = 14),
  axis.text.x = element_text(size = 15*(sf+.5), colour = "black",),
  axis.text.y = element_text(size = 15*(sf+.5), colour = "black"),
  
  
  # panel
  panel.grid.major = element_blank(),
  panel.background = element_blank(),
  # panel.background = element_rect(fill="transparent"),
  # panel.border = element_rect(fill="transparent"),
  # strip shades (reco rectagles)
   # strip.background = element_blank(),
  
  # legend
  # legend.position = "top",
  legend.key = element_rect(colour = "transparent", fill="transparent"),
  # axis.ticks = element_blank(),
  # legend.k
  #legend.direction = "horizontal",

  legend.text = element_text(size = 16*(sf+.5)),
  legend.title= element_text(size = 16*(sf+.5)),
  # legend.title = element_text(size = 10*(sf+.3)),
  # legend line tends to be really small in print so adjust here
  # legend.key.height = unit(.5, "cm"),
  # legend.key.width = unit(2, "cm"),
  
  #legend.text = element_text(size = 10*sf),
  # legend.title=element_blank(),
  #legend.text = element_blank(),
  #axis.ticks = element_blank(),
)


```



# recreate heatmap AUS

```{r}

chool_talk$AU_NM <-
as.data.frame(res_k3@fit@H)%>%
  mutate(component = as.factor(1:n()))%>%
  gather(AU, coef, -component)%>%
  group_by(component)%>%
  mutate(max_comp = max(coef),
         coef = coef/max_comp,
         # AU = substring(AU,8,40),
         AU_code = substring(AU,4,5)
         )%>%
  arrange(AU_code)%>%
  ggplot(aes(component,AU, fill = coef))+
  geom_tile()+
  coord_cartesian(expand = FALSE)+
  # make_fullsize()
  
  theme_classic()+
  # scale_fill_viridis_()
    scale_fill_viridis_c(option = "magma", breaks=c(0,.5,1),
                       limits =c(0,1))+#)+
  xlab("Component (K)")+

  scale_x_discrete(position = "top")+
guides(fill=guide_colorbar(ticks.colour = NA))
  






```


```{r}

triangle_plots$delaunay_tri_comp3


# require()


# I is for italic
labels <- c(
  `1` = "<img src='https://github.com/hcuve/halo_faces/raw/main/ggtextimgs/comp1noct.png'
    width='100' /><br> 1",
  `2` = "<img src='https://github.com/hcuve/halo_faces/raw/main/ggtextimgs/comp2noct.png'
    width='100' /><br> 2",
  `3` = "<img src='https://github.com/hcuve/halo_faces/raw/main/ggtextimgs/comp3noct.png'
    width='100' /><br> 3"
)
# ggtext::


chool_talk$AU_NM<-  chool_talk$AU_NM +
  xlab("Component (K)")+
    scale_x_discrete(
      position = "top",
    # name = NULL,
    labels = labels
  ) +
  theme(
     axis.text.x.top = element_markdown(color = "black", size = 16*(sf+.5)),
     axis.title.x = element_text(size = 16*(sf+.5)),#update this,
     axis.title.y = element_text(size = 16*(sf+.5)),#update this,
     axis.text.x = element_text(size = 15*(sf+.5), colour = "black"),
  axis.text.y = element_text(size = 15*(sf+.5), colour = "black"),
  legend.key.height = unit(1, "cm"),
  legend.key.width = unit(.7, "cm"),
  legend.text = element_text(size = 16*(sf+.5)),
  legend.title= element_text(size = 16*(sf+.5))
    # axis.text.x = element_markdown(color = "black", size = 15),
       # axis.text.x.top = element_markdown(size = 8, lineheight = 1.05)
     # axis.title.x = element_markdown(color = "black", size = 20)
  )


chool_talk$AU_NM



```







  
```{r}
# chool_talk$AU_NM2<- chool_talk$AU_NM+
#   theme(axis.text.y = element_blank())+
#   ylab("Motor Action Unit")

chool_talk$AU_NM2
chool_talk$ts_smooth

ggsave("AU_NMF2.tiff",chool_talk$AU_NM2, device = "tiff", width = 5, height = 8, dpi = 800)

chool_talk$ts_hm

```


timeseries
```{r}
chool_talk$ts_smooth <- df_OF_output_AUsW_unblind_posed_binned[,c(1,2:3,9:10,34:36)]%>%
  gather(component, coef,-bin_frame, -subject, -expression, -drug.placebo,-filename)%>%
  
      group_by(filename,component,subject,expression)%>%
  mutate(coef_max = max(coef),
         coef = coef/coef_max) %>%
  
  group_by(expression, component, bin_frame,subject)%>%
  summarise_if(is.numeric, mean, na.rm = TRUE)%>%
  # ggplot(aes(Dataset, coef , color = Dataset, fill = Dataset))+
  ggplot(aes(bin_frame, coef, color = component))+
  geom_smooth()+
  # geom_smooth(aes(group = subject), se = F)+
  # stat_summary(aes(group = subject, color = component), geom = "smooth")+
  facet_grid(~expression )+
  theme_classic()+
   xlab("time bin")+
  p$graphstyle_int+
   scale_color_viridis_d(option = "magma")


chool_talk$ts_smooth

# just components
paper_plots$posed_comp_ts<-
df_OF_output_AUsW_unblind_posed_binned[,c(1,2:3,9:10,34:36)]%>%
  gather(component, coef,-bin_frame, -subject, -expression, -drug.placebo,-filename)%>%
  
      group_by(filename,component)%>%
  mutate(coef_max = max(coef),
         coef = coef/coef_max) %>%
  
  group_by(component, bin_frame,subject)%>%
  summarise_if(is.numeric, mean, na.rm = TRUE)%>%
  # ggplot(aes(Dataset, coef , color = Dataset, fill = Dataset))+
  ggplot(aes(bin_frame, coef, color = component))+
  geom_smooth()+
  # geom_smooth(aes(group = subject), se = F)+
  # stat_summary(aes(group = subject, color = component), geom = "smooth")+
  # facet_grid(~expression )+
  theme_classic()+
   xlab("time bin")+
  p$graphstyle_int+
   scale_color_viridis_d(option = "magma")
paper_plots$posed_comp_ts
```


   
chool_talk$ts_smooth


heatmap + timeseries + timeseries heatmap
```{r}
chool_talk$ts_hm <-
df_OF_output_AUsW_unblind_posed_binned[,c(1,2:3,9:10,34:36)]%>%
  gather(component, coef,-bin_frame, -subject, -expression, -drug.placebo,-filename)%>%
  group_by(filename,component)%>%
  mutate(coef_max = max(coef),
         coef = coef/coef_max)%>%
  # mutate_at(c("coef"), scale)%>%
  group_by(component, expression, bin_frame)%>%

  summarise_if(is.numeric, mean, na.rm = TRUE)%>%
  # ggplot(aes(Dataset, coef , color = Dataset, fill = Dataset))+
  ggplot(aes(bin_frame, component, fill= coef))+
  geom_tile()+
  xlab("time bin")+
  scale_fill_viridis_c(option = "magma", breaks=c(0,.5,1),
                       limits =c(0,1))+#)+
  # geom_smooth(se = F)+
  facet_grid(~expression)+
  theme_classic()+
  p$graphstyle_int+
  guides(fill=guide_colorbar(ticks.colour = NA))+
  theme(panel.spacing = unit(1, "cm"))
   # guides(fill = guide_colourbar(barwidth = 1.5,ticks.colour = NA, barheight = 5))

chool_talk$ts_hm 

paper_plots$posed_comp_ts_hm<-
df_OF_output_AUsW_unblind_posed_binned[,c(1,2:3,9:10,34:36)]%>%
  gather(component, coef,-bin_frame, -subject, -expression, -drug.placebo,-filename)%>%
  group_by(filename,component)%>%
  mutate(coef_max = max(coef),
         coef = coef/coef_max)%>%
  # mutate_at(c("coef"), scale)%>%
  group_by(component, bin_frame)%>%

  summarise_if(is.numeric, mean, na.rm = TRUE)%>%
  # ggplot(aes(Dataset, coef , color = Dataset, fill = Dataset))+
  ggplot(aes(bin_frame, component, fill= coef))+
  geom_tile()+
  xlab("time bin")+
  scale_fill_viridis_c(option = "magma", breaks=c(0,.5,1),
                       limits =c(0,1))+#)+
  # facet_grid(~expression)+
  theme_classic()+
  p$graphstyle_int +
  guides(fill=guide_colorbar(ticks.colour = NA))+
  theme(panel.spacing = unit(1, "cm"))
   # guides(fill = guide_colourbar(barwidth = 1.5,ticks.colour = NA, barheight = 5))

chool_talk$ts_hm 

paper_plots$posed_comp_ts_hm

```

library(patchwork)
```{r}
# chool_talk$ts_nmf_heat_NMF <- (chool_talk$ts_smooth/
#   chool_talk$ts_hm)+
#   plot_layout(heights = c(1,2))

# soace out values in x axis
chool_talk$ts_smooth <- chool_talk$ts_smooth + scale_x_continuous(breaks=c(0,50,100), limits = c(0, 100))+
    scale_y_continuous(breaks=c(0,.5,1), limits = c(0, 1))+
  theme(panel.spacing.x = unit(1.5, "lines"))+
  theme(legend.position = "top")

chool_talk$ts_smooth

test4pot5


# chool_talk$ts_smooth <- test4pot5 + scale_x_continuous(breaks=c(0,50,100), limits = c(0, 100))+
#     scale_y_continuous(breaks=c(0,.5,1), limits = c(0, 1))+
#   theme(panel.spacing.x = unit(1.5, "lines"))+
#   theme(legend.position = "top")

chool_talk$ts_hm <- chool_talk$ts_hm +scale_x_continuous(breaks=c(0,50,100), limits = c(0, 100))+
  theme(panel.spacing.x = unit(1.5, "lines"))

chool_talk$ts_hm 
# components
paper_plots$posed_comp_ts <- paper_plots$posed_comp_ts + scale_x_continuous(breaks=c(0,50,100), limits = c(0, 100))+
    # scale_y_continuous(breaks=c(0,.5,1), limits = c(0, 1))+
  theme(panel.spacing.x = unit(1.5, "lines"))+
  theme(legend.position = "top")

paper_plots$posed_comp_ts_hm <-paper_plots$posed_comp_ts_hm +scale_x_continuous(breaks=c(0,50,100), limits = c(0, 100))+
  theme(panel.spacing.x = unit(1.5, "lines"))

chool_talk$ts_smooth
chool_talk$ts_hm 
paper_plots$posed_comp_ts
paper_plots$posed_comp_ts_hm

```



````{r}

require(patchwork)

paper_plots$NMF_panel_posed

test
library(patchwork)
# make sure to flip 1 to 3

 triangle_plots$delaunay_tri_comp1.1 <- triangle_plots$delaunay_tri_comp1 +
theme_void()+theme(legend.position = "none",
                     axis.title.x=element_text(size = 16*(sf+.5), margin=margin(0,5,0,0)))+xlab("Comp1")


triangle_plots$delaunay_tri_comp2.1<- triangle_plots$delaunay_tri_comp2+
theme_void()+theme(legend.position = "none",
                     axis.title.x=element_text(size = 16*(sf+.5), margin=margin(0,5,0,0)))+xlab("Comp2")
 
triangle_plots$delaunay_tri_comp3.1<- triangle_plots$delaunay_tri_comp3+
theme_void()+theme(legend.position = "none",
                     axis.title.x=element_text(size = 16*(sf+.5), margin=margin(0,5,0,0)))+xlab("Comp3")


comp1_2_3<- triangle_plots$delaunay_tri_comp1.1 /
        triangle_plots$delaunay_tri_comp2.1/
       triangle_plots$delaunay_tri_comp3.1

comp1_2_3

```

```{r}
paper_plots$posed_comp_ts
paper_plots$posed_comp_ts_hm

ts_ts_hm_compns<- (paper_plots$posed_comp_ts/paper_plots$posed_comp_ts_hm)
ts_ts_hm_compns


comp1_2_3
```






```{r}

save.image("~/Library/CloudStorage/GoogleDrive-helioclemente.c@gmail.com/My Drive/2022 - University Of Birmingham/HaloStudy/Data/halofacestudyOCT2023.RData")
```
